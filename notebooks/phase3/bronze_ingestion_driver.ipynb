{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1493ea9-5e77-4399-8cd1-ede65f47c3d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: bronze_ingestion_driver (Loop Through All Metadata Entities)\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Add utility paths\n",
    "sys.path.append(\"/dbfs/mnt/scripts/libs\")\n",
    "from parse_dq_rules import parse_dq_rules\n",
    "from schema_validator import validate_schema\n",
    "from dq_engine import apply_dq_rules\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# üì• Load all active entities from metadata table\n",
    "# -----------------------------------------------------\n",
    "jdbc_url = \"jdbc:sqlserver://sql-retailverse-dev-server.database.windows.net:1433;database=retailverse_metadata_db\"\n",
    "jdbc_properties = {\n",
    "    \"user\": dbutils.secrets.get(scope=\"retailverse-secret\", key=\"sql-user\"),\n",
    "    \"password\": dbutils.secrets.get(scope=\"retailverse-secret\", key=\"sql-password\"),\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "entities_query = \"(SELECT entity_name, source_path, dq_rules, partition_column, expected_schema FROM retailverse_metadata.metadata_control WHERE active = 1) AS metadata\"\n",
    "metadata_df = spark.read.jdbc(url=jdbc_url, table=entities_query, properties=jdbc_properties)\n",
    "metadata_list = metadata_df.collect()\n",
    "\n",
    "if not metadata_list:\n",
    "    raise Exception(\"‚ùå No active entities found in metadata table.\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# üîÅ Loop over each entity for ingestion\n",
    "# -----------------------------------------------------\n",
    "for row in metadata_list:\n",
    "    entity_name = row[\"entity_name\"]\n",
    "    source_path = row[\"source_path\"]\n",
    "    dq_rules_raw = row[\"dq_rules\"]\n",
    "    partition_column = row[\"partition_column\"]\n",
    "    expected_schema_json = row[\"expected_schema\"]\n",
    "\n",
    "    print(f\"üöÄ Ingesting entity: {entity_name}\")\n",
    "    print(f\"üìÇ Source Path: {source_path}\")\n",
    "    print(f\"üì¶ Partition Column: {partition_column}\")\n",
    "\n",
    "    bronze_output_path = f\"/mnt/bronze/{entity_name}/\"\n",
    "    quarantine_path = f\"/mnt/quarantine/{entity_name}/\"\n",
    "\n",
    "    # Audit logging setup\n",
    "    run_id = str(uuid.uuid4())\n",
    "    start_time = datetime.now()\n",
    "    status = \"SUCCESS\"\n",
    "    row_count = 0\n",
    "    error_message = \"\"\n",
    "\n",
    "    try:\n",
    "        files = [f.name for f in dbutils.fs.ls(source_path) if f.name.endswith(\".csv\")]\n",
    "        if not files:\n",
    "            print(f\"‚ö†Ô∏è No CSV files found in: {source_path}\")\n",
    "            continue\n",
    "\n",
    "        df_raw = spark.read.option(\"header\", True).csv(source_path)\n",
    "\n",
    "        # Load expected schema and DQ rules\n",
    "        expected_schema_dict = json.loads(expected_schema_json)\n",
    "        dq_rules = parse_dq_rules(dq_rules_raw)\n",
    "\n",
    "        # Validate schema\n",
    "        df_validated = validate_schema(df_raw, expected_schema_dict)\n",
    "\n",
    "        # Apply DQ rules\n",
    "        invalid_rows = spark.createDataFrame([], df_validated.schema)\n",
    "        for rule in dq_rules.get(\"rules\", []):\n",
    "            column = rule.get(\"column\")\n",
    "            rule_type = rule.get(\"rule\")\n",
    "            if rule_type == \"not_null\":\n",
    "                failed = df_validated.filter(col(column).isNull()).withColumn(\"error_reason\", lit(f\"{column} is null\"))\n",
    "                invalid_rows = invalid_rows.unionByName(failed, allowMissingColumns=True)\n",
    "                df_validated = df_validated.filter(col(column).isNotNull())\n",
    "            elif rule_type == \"positive_number\":\n",
    "                failed = df_validated.filter((col(column).cast(\"double\") <= 0) | (col(column).isNull())).withColumn(\"error_reason\", lit(f\"{column} is not positive\"))\n",
    "                invalid_rows = invalid_rows.unionByName(failed, allowMissingColumns=True)\n",
    "                df_validated = df_validated.filter(col(column).cast(\"double\") > 0)\n",
    "            elif rule_type == \"email_format\":\n",
    "                failed = df_validated.filter(~col(column).rlike(\"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\")).withColumn(\"error_reason\", lit(f\"{column} failed email_format\"))\n",
    "                invalid_rows = invalid_rows.unionByName(failed, allowMissingColumns=True)\n",
    "                df_validated = df_validated.filter(col(column).rlike(\"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\"))\n",
    "\n",
    "        # Quarantine invalid rows\n",
    "        if invalid_rows.count() > 0:\n",
    "            invalid_rows.write.mode(\"append\").parquet(quarantine_path)\n",
    "            print(f\"‚ö†Ô∏è Quarantined {invalid_rows.count()} invalid rows for entity: {entity_name}\")\n",
    "\n",
    "        # Write valid rows to Bronze\n",
    "        if partition_column and partition_column in df_validated.columns:\n",
    "            df_validated.write.mode(\"overwrite\").partitionBy(partition_column).parquet(bronze_output_path)\n",
    "        else:\n",
    "            df_validated.write.mode(\"overwrite\").parquet(bronze_output_path)\n",
    "\n",
    "        row_count = df_validated.count()\n",
    "        print(f\"‚úÖ Ingestion completed for: {entity_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        status = \"FAILED\"\n",
    "        error_message = str(e)\n",
    "        print(f\"‚ùå Failed ingestion for {entity_name}: {error_message}\")\n",
    "\n",
    "    finally:\n",
    "        end_time = datetime.now()\n",
    "        audit_data = [(run_id, entity_name, start_time, end_time, status, row_count, error_message)]\n",
    "        audit_schema = \"run_id STRING, entity_name STRING, start_time TIMESTAMP, end_time TIMESTAMP, status STRING, row_count INT, error_message STRING\"\n",
    "        audit_df = spark.createDataFrame(audit_data, schema=audit_schema)\n",
    "\n",
    "        audit_df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", \"retailverse_metadata.audit_logs\") \\\n",
    "            .option(\"user\", jdbc_properties[\"user\"]) \\\n",
    "            .option(\"password\", jdbc_properties[\"password\"]) \\\n",
    "            .option(\"driver\", jdbc_properties[\"driver\"]) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "\n",
    "        audit_df.write \\\n",
    "            .mode(\"append\") \\\n",
    "            .partitionBy(\"entity_name\", \"run_id\") \\\n",
    "            .parquet(\"/mnt/logs/audit_logs/\")\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingestion_driver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
